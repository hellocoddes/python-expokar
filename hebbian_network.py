# -*- coding: utf-8 -*-
"""6

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JJPI5_VtxqMlRqu9eXHSwu_Gepa6YZyY
"""

import numpy as np
import matplotlib.pyplot as plt
from itertools import product

class HebbianNeuron:
    """
    Implementation of Hebbian learning for basic logic gates.

    Hebbian learning principle: "Neurons that fire together, wire together."
    When input and output are both active, the connection between them strengthens.
    """
    def __init__(self, input_size, learning_rate=0.1, use_bias=True):
        """
        Initialize a neuron with Hebbian learning capability.

        Parameters:
        - input_size: Number of inputs to the neuron
        - learning_rate: Rate at which weights are updated
        - use_bias: Whether to include a bias term
        """
        self.input_size = input_size
        self.learning_rate = learning_rate
        self.use_bias = use_bias

        # Initialize weights with small random values
        if use_bias:
            # Add an extra weight for the bias input (which is always 1)
            self.weights = np.random.randn(input_size + 1) * 0.1
        else:
            self.weights = np.random.randn(input_size) * 0.1

        # To store training history
        self.weight_history = [self.weights.copy()]
        self.errors = []

    def _add_bias(self, x):
        """Add bias term to input."""
        if self.use_bias:
            if len(x.shape) == 1:  # Single input
                return np.insert(x, 0, 1)
            else:  # Batch of inputs
                bias_column = np.ones((x.shape[0], 1))
                return np.hstack((bias_column, x))
        return x

    def activate(self, x):
        """
        Compute the neuron's output for the given input.

        Parameters:
        - x: Input vector or matrix

        Returns:
        - Binary output (0 or 1)
        """
        x_with_bias = self._add_bias(np.array(x))
        net_input = np.dot(x_with_bias, self.weights)
        # Step activation function
        return (net_input >= 0).astype(int)

    # Correct implementation for the train_hebbian method
    def train_hebbian(self, X, y, epochs=100):
        """
        Train the neuron using Hebbian learning rule.

        Parameters:
        - X: Training inputs
        - y: Target outputs
        - epochs: Number of training epochs

        Returns:
        - List of errors during training
        """
        X = np.array(X)
        y = np.array(y)
        n_samples = X.shape[0]

        print("Initial weights:", self.weights)

        for epoch in range(epochs):
            errors = 0

            # Process each training example
            for i in range(n_samples):
                x_i = X[i]
                y_i = y[i]

                # Add bias
                x_i_with_bias = self._add_bias(x_i)

                # Get current prediction
                y_pred = self.activate(x_i)

                # Update errors
                if y_pred != y_i:
                    errors += 1

                # Hebbian update rule: weight change proportional to input * output
                # For scalar output, we multiply each input with the output
                delta_w = self.learning_rate * x_i_with_bias * y_i

                # If both input and target output are 1, increase weight
                # Otherwise, no change
                self.weights += delta_w

            # Store weights and errors for history
            self.weight_history.append(self.weights.copy())
            self.errors.append(errors / n_samples)

            # Print progress
            if epoch % 10 == 0 or epoch == epochs-1:
                print(f"Epoch {epoch}: Error rate = {errors/n_samples:.4f}, Weights = {self.weights}")

            # Early stopping if perfect classification
            if errors == 0:
                print(f"Converged at epoch {epoch}")
                break

        return self.errors

    def train_perceptron(self, X, y, epochs=100):
        """
        Train using the perceptron learning rule for comparison.

        Parameters:
        - X: Training inputs
        - y: Target outputs
        - epochs: Number of training epochs

        Returns:
        - List of errors during training
        """
        X = np.array(X)
        y = np.array(y)
        n_samples = X.shape[0]

        print("Initial weights:", self.weights)

        for epoch in range(epochs):
            errors = 0

            # Process each training example
            for i in range(n_samples):
                x_i = X[i]
                y_i = y[i]

                # Add bias
                x_i_with_bias = self._add_bias(x_i)

                # Get current prediction
                y_pred = self.activate(x_i)

                # Update errors
                if y_pred != y_i:
                    errors += 1

                    # Perceptron update rule: weight change proportional to error * input
                    delta_w = self.learning_rate * (y_i - y_pred) * x_i_with_bias
                    self.weights += delta_w

            # Store weights and errors for history
            self.weight_history.append(self.weights.copy())
            self.errors.append(errors / n_samples)

            # Print progress
            if epoch % 10 == 0 or epoch == epochs-1:
                print(f"Epoch {epoch}: Error rate = {errors/n_samples:.4f}, Weights = {self.weights}")

            # Early stopping if perfect classification
            if errors == 0:
                print(f"Converged at epoch {epoch}")
                break

        return self.errors

    def plot_decision_boundary(self, X, y, title="Decision Boundary"):
        """
        Plot the decision boundary for 2-input neurons.

        Parameters:
        - X: Input features
        - y: Target values
        - title: Plot title
        """
        if X.shape[1] != 2:
            print("Decision boundary plotting only works for 2-input neurons")
            return

        plt.figure(figsize=(10, 6))

        # Plot training points
        for label in np.unique(y):
            mask = (y == label)
            plt.scatter(X[mask, 0], X[mask, 1], label=f"Class {label}",
                       marker="o" if label == 1 else "x",
                       s=100, alpha=0.8)

        # Create a mesh grid for the decision boundary
        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),
                          np.linspace(y_min, y_max, 100))

        # Make predictions on the mesh grid
        grid = np.c_[xx.ravel(), yy.ravel()]
        Z = self.activate(grid)
        Z = Z.reshape(xx.shape)

        # Plot decision boundary
        plt.contourf(xx, yy, Z, alpha=0.3)

        # Add weight vector (normal to the decision boundary)
        if self.use_bias:
            w1, w2 = self.weights[1], self.weights[2]
            bias = self.weights[0]

            # Draw line: w1*x + w2*y + bias = 0
            if w2 != 0:
                x_line = np.array([x_min, x_max])
                y_line = (-bias - w1 * x_line) / w2
                plt.plot(x_line, y_line, 'r-', lw=2, label="Decision Boundary")
            else:
                # Vertical line if w2 = 0
                x_line = -bias / w1
                plt.axvline(x=x_line, color='r', lw=2, label="Decision Boundary")

            # Draw weight vector
            plt.arrow(0, 0, w1, w2, head_width=0.1, head_length=0.1, fc='k', ec='k', label="Weight Vector")

        plt.xlim(x_min, x_max)
        plt.ylim(y_min, y_max)
        plt.xlabel("Input 1")
        plt.ylabel("Input 2")
        plt.title(title)
        plt.legend()
        plt.grid(True)
        plt.show()

    def plot_training_progress(self):
        """Plot the training progress (error rate and weight changes)."""
        plt.figure(figsize=(15, 5))

        # Plot error rate
        plt.subplot(1, 2, 1)
        plt.plot(self.errors, 'b-')
        plt.xlabel("Epoch")
        plt.ylabel("Error Rate")
        plt.title("Training Error")
        plt.grid(True)

        # Plot weight changes
        plt.subplot(1, 2, 2)
        weight_history = np.array(self.weight_history)
        for i in range(weight_history.shape[1]):
            plt.plot(weight_history[:, i], label=f"w{i}")
        plt.xlabel("Epoch")
        plt.ylabel("Weight Value")
        plt.title("Weight Evolution")
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

def generate_logic_gate_data(gate_type):
    """
    Generate training data for a specific logic gate.

    Parameters:
    - gate_type: Type of logic gate ('AND', 'OR', 'NOT', 'NAND', 'NOR', 'XOR')

    Returns:
    - X: Input data
    - y: Target outputs
    """
    if gate_type == 'NOT':
        X = np.array([[0], [1]])
        if gate_type == 'NOT':
            y = np.array([1, 0])
    else:
        # Generate all combinations of 2 binary inputs
        X = np.array(list(product([0, 1], repeat=2)))

        # Generate outputs based on gate type
        if gate_type == 'AND':
            y = np.logical_and(X[:, 0], X[:, 1]).astype(int)
        elif gate_type == 'OR':
            y = np.logical_or(X[:, 0], X[:, 1]).astype(int)
        elif gate_type == 'NAND':
            y = np.logical_not(np.logical_and(X[:, 0], X[:, 1])).astype(int)
        elif gate_type == 'NOR':
            y = np.logical_not(np.logical_or(X[:, 0], X[:, 1])).astype(int)
        elif gate_type == 'XOR':
            y = np.logical_xor(X[:, 0], X[:, 1]).astype(int)
        else:
            raise ValueError(f"Unknown gate type: {gate_type}")

    return X, y

def hebbian_vs_perceptron(gate_type, epochs=100):
    """
    Compare Hebbian learning with Perceptron learning for a logic gate.

    Parameters:
    - gate_type: Type of logic gate to learn
    - epochs: Maximum number of training epochs
    """
    # Generate data
    X, y = generate_logic_gate_data(gate_type)

    # Create neurons
    input_size = X.shape[1]
    hebbian_neuron = HebbianNeuron(input_size, learning_rate=0.1)
    perceptron_neuron = HebbianNeuron(input_size, learning_rate=0.1)

    # Train both neurons
    print(f"\n{'-'*50}")
    print(f"Training {gate_type} gate with Hebbian learning:")
    hebbian_errors = hebbian_neuron.train_hebbian(X, y, epochs)

    print(f"\n{'-'*50}")
    print(f"Training {gate_type} gate with Perceptron learning:")
    perceptron_errors = perceptron_neuron.train_perceptron(X, y, epochs)

    # Test final accuracy
    hebbian_preds = hebbian_neuron.activate(X)
    perceptron_preds = perceptron_neuron.activate(X)

    hebbian_accuracy = np.mean(hebbian_preds == y)
    perceptron_accuracy = np.mean(perceptron_preds == y)

    print(f"\n{'-'*50}")
    print(f"Results for {gate_type} gate:")
    print(f"Hebbian accuracy: {hebbian_accuracy:.4f}")
    print(f"Perceptron accuracy: {perceptron_accuracy:.4f}")

    # Plot decision boundaries (only for 2-input gates)
    if X.shape[1] == 2:
        hebbian_neuron.plot_decision_boundary(X, y, f"Hebbian Learning: {gate_type} Gate")
        perceptron_neuron.plot_decision_boundary(X, y, f"Perceptron Learning: {gate_type} Gate")

    # Plot training progress
    plt.figure(figsize=(12, 5))
    plt.plot(hebbian_errors, 'b-', label="Hebbian")
    plt.plot(perceptron_errors, 'r-', label="Perceptron")
    plt.xlabel("Epoch")
    plt.ylabel("Error Rate")
    plt.title(f"Learning Comparison for {gate_type} Gate")
    plt.legend()
    plt.grid(True)
    plt.show()

    # Show training details
    hebbian_neuron.plot_training_progress()
    perceptron_neuron.plot_training_progress()

    return hebbian_neuron, perceptron_neuron

def test_all_gates():
    """Test Hebbian learning on all standard logic gates."""
    gates = ['AND', 'OR', 'NAND', 'NOR']

    results = {}
    for gate in gates:
        print(f"\n{'='*60}")
        print(f"Testing {gate} gate:")
        print(f"{'='*60}")

        hebbian, perceptron = hebbian_vs_perceptron(gate)
        results[gate] = {
            'hebbian': hebbian,
            'perceptron': perceptron
        }

    # Try XOR as well (should fail for both)
    print(f"\n{'='*60}")
    print("Testing XOR gate (expected to fail for single-layer neurons):")
    print(f"{'='*60}")
    hebbian_vs_perceptron('XOR')

    return results

# Run the experiments
if __name__ == "__main__":
    np.random.seed(42)  # For reproducibility

    print("Hebbian Learning for Logic Gates")
    print("================================")

    results = test_all_gates()

    # Show a custom example with different learning rates
    print("\nCustom Example: OR gate with different learning rates")
    X, y = generate_logic_gate_data('OR')

    learning_rates = [0.01, 0.1, 0.5]
    plt.figure(figsize=(12, 5))

    for lr in learning_rates:
        neuron = HebbianNeuron(2, learning_rate=lr)
        print(f"\nTraining with learning rate: {lr}")
        errors = neuron.train_hebbian(X, y, epochs=50)
        plt.plot(errors, label=f"LR={lr}")

    plt.xlabel("Epoch")
    plt.ylabel("Error Rate")
    plt.title("Effect of Learning Rate on Hebbian Learning")
    plt.legend()
    plt.grid(True)
    plt.show()

