# -*- coding: utf-8 -*-
"""5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JJPI5_VtxqMlRqu9eXHSwu_Gepa6YZyY
"""

# SVM Implementation with Kernel Tricks
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Generate a sample dataset (non-linearly separable)
# We'll use the make_moons dataset from sklearn which creates a binary classification
# problem with a moon-shaped decision boundary
X, y = datasets.make_moons(n_samples=200, noise=0.15, random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 2. Visualization function to see how SVM with different kernels classify the data
def plot_decision_boundary(models, titles, X, y, X_test=None, y_test=None):
    # Set up the figure size
    plt.figure(figsize=(15, 10))

    # Define the mesh grid
    h = 0.02  # Step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

    # Plot each model's decision boundary
    for i, (model, title) in enumerate(zip(models, titles)):
        plt.subplot(2, 2, i + 1)

        # Plot the decision boundary
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        plt.contourf(xx, yy, Z, alpha=0.3)

        # Plot the training points
        plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)

        # Plot the test points if provided
        if X_test is not None and y_test is not None:
            plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test,
                       marker='^', edgecolors='r', cmap=plt.cm.Paired, alpha=0.6)

        plt.title(title)
        plt.xlabel('Feature 1')
        plt.ylabel('Feature 2')

        # Display accuracy on the plot
        if X_test is not None and y_test is not None:
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            plt.text(x_min + 0.2, y_min + 0.2, f'Accuracy: {accuracy:.2f}',
                    bbox=dict(facecolor='white', alpha=0.5))

    plt.tight_layout()
    plt.show()

# 3. Training SVM with different kernels
# a. Linear SVM
svm_linear = SVC(kernel='linear', C=1.0, random_state=42)
svm_linear.fit(X_train_scaled, y_train)

# b. Polynomial Kernel SVM
svm_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)
svm_poly.fit(X_train_scaled, y_train)

# c. RBF Kernel SVM (Gaussian)
svm_rbf = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42)
svm_rbf.fit(X_train_scaled, y_train)

# d. Sigmoid Kernel SVM
svm_sigmoid = SVC(kernel='sigmoid', gamma='scale', C=1.0, random_state=42)
svm_sigmoid.fit(X_train_scaled, y_train)

# 4. Evaluate the models
models = [svm_linear, svm_poly, svm_rbf, svm_sigmoid]
titles = ['Linear Kernel', 'Polynomial Kernel', 'RBF Kernel', 'Sigmoid Kernel']

# Plot decision boundaries
plot_decision_boundary(models, titles, X_train_scaled, y_train, X_test_scaled, y_test)

# 5. Detailed evaluation of each model
def evaluate_model(model, X_train, y_train, X_test, y_test, name):
    print(f"=== {name} SVM ===")

    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # Training performance
    train_accuracy = accuracy_score(y_train, y_train_pred)
    print(f"Training Accuracy: {train_accuracy:.4f}")

    # Testing performance
    test_accuracy = accuracy_score(y_test, y_test_pred)
    print(f"Testing Accuracy: {test_accuracy:.4f}")

    # Confusion matrix
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_test_pred))

    # Classification report
    print("Classification Report:")
    print(classification_report(y_test, y_test_pred))

    # Number of support vectors
    print(f"Number of Support Vectors: {model.n_support_}")
    print("\n")

# Evaluate each model
for model, name in zip(models, titles):
    evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, name)

# 6. Effects of hyperparameters in RBF Kernel SVM
def train_rbf_models(C_values, gamma_values, X_train, y_train):
    models = []
    params = []

    for C in C_values:
        for gamma in gamma_values:
            svm = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)
            svm.fit(X_train, y_train)
            models.append(svm)
            params.append(f'C={C}, Î³={gamma}')

    return models, params

# Different C and gamma values to explore
C_values = [0.1, 1, 10]
gamma_values = [0.1, 1, 'scale']

# Train models with different parameters
rbf_models, rbf_params = train_rbf_models(C_values, gamma_values, X_train_scaled, y_train)

# Create titles for the plots
rbf_titles = [f'RBF Kernel ({param})' for param in rbf_params]

# Plot all models (only showing first 4 to avoid cluttering)
plot_decision_boundary(rbf_models[:4], rbf_titles[:4], X_train_scaled, y_train, X_test_scaled, y_test)

# 7. Custom implementation of kernel functions
def linear_kernel(x1, x2):
    return np.dot(x1, x2)

def polynomial_kernel(x1, x2, degree=3):
    return (np.dot(x1, x2) + 1) ** degree

def rbf_kernel(x1, x2, gamma=1.0):
    return np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)

def sigmoid_kernel(x1, x2, gamma=1.0, coef0=0.0):
    return np.tanh(gamma * np.dot(x1, x2) + coef0)

# Example of computing kernel matrix (Gram matrix) for a small dataset
def compute_gram_matrix(X, kernel_func, **kernel_params):
    n_samples = X.shape[0]
    gram_matrix = np.zeros((n_samples, n_samples))

    for i in range(n_samples):
        for j in range(n_samples):
            gram_matrix[i, j] = kernel_func(X[i], X[j], **kernel_params)

    return gram_matrix

# Demonstrate kernel trick with a small subset of data
X_subset = X_train_scaled[:5]

print("Small subset of data:")
print(X_subset)
print("\nLinear Kernel Matrix:")
print(compute_gram_matrix(X_subset, linear_kernel))
print("\nPolynomial Kernel Matrix (degree=3):")
print(compute_gram_matrix(X_subset, polynomial_kernel, degree=3))
print("\nRBF Kernel Matrix (gamma=1.0):")
print(compute_gram_matrix(X_subset, rbf_kernel, gamma=1.0))

# 8. Summary of the SVM with Kernel Tricks performance
print("\n==== Summary of SVM Kernel Performance ====")
for model, name in zip(models, titles):
    y_test_pred = model.predict(X_test_scaled)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    print(f"{name}: Test Accuracy = {test_accuracy:.4f}, Support Vectors = {sum(model.n_support_)}")

